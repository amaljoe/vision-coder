# Image-Conditioned Code Generation using Reinforcement Learning with Verifiable Rewards (RLVR)

## ğŸ“Œ Overview

This project explores whether **Reinforcement Learning with Verifiable
Rewards (RLVR)** can improve **image-conditioned code generation**,
specifically:

> **Input:** UI screenshot or PDF page\
> **Output:** HTML + CSS that visually reproduces the input

Rather than relying purely on supervised fine-tuning (SFT), we train a
**vision-language model (VLM)** using rewards computed from
**automatically verifiable signals** such as rendering similarity and
structural correctness.

------------------------------------------------------------------------

## ğŸ¯ Motivation

Recent work shows RLVR significantly improves reasoning in text-only
tasks (math, coding). However:

-   RLVR for **vision-conditioned generation** remains underexplored\
-   Practical demand for **UI/PDF â†’ HTML/CSS** systems is growing\
-   These tasks require **visual understanding + structured generation**

This project investigates whether RLVR can:

âœ… Improve layout fidelity\
âœ… Reduce malformed HTML/CSS\
âœ… Encourage structured reasoning\
âœ… Work with minimal human annotation

------------------------------------------------------------------------

## ğŸ§© Problem Statement

We train a model that:

-   Accepts an **image** (UI mockup / PDF page)\
-   Generates **HTML + CSS**\
-   Receives rewards based on **verifiable similarity metrics**

**Goal:** Produce code that renders as close as possible to the input
image.

------------------------------------------------------------------------

## ğŸ§  Proposed Approach

### 1ï¸âƒ£ Base Pipeline

1.  Input image â†’ Vision-Language Model\
2.  Model generates HTML + CSS\
3.  Code is rendered (headless browser)\
4.  Reward computed via:
    -   Visual similarity
    -   Structural validity
    -   Layout consistency

------------------------------------------------------------------------

### 2ï¸âƒ£ Reinforcement Learning with Verifiable Rewards

Instead of token-level supervision:

-   Rewards derived from automatic checkers\
-   No human scoring required\
-   Encourages global correctness rather than token matching

------------------------------------------------------------------------

## ğŸ† Reward Design

Potential reward components:

### âœ… Visual Fidelity

-   SSIM / LPIPS / Pixel similarity\
-   Layout alignment

### âœ… HTML/CSS Validity

-   Syntax correctness\
-   Proper tag nesting

### âœ… Structural Accuracy

-   DOM tree similarity\
-   Layout block consistency\
-   Reading order correctness

### âœ… PDF-Specific Constraints

-   Table row/column preservation\
-   Bounding box alignment

------------------------------------------------------------------------

## ğŸ§± Candidate Base Models

-   Qwen 3 VL Thinking 4B\
-   Qwen 3 VL Instruct 8B\
-   Unsloth Devstral 24B\
-   DeepSeek VL2 27B

Selection criteria:

âœ” Strong multimodal understanding\
âœ” Efficient RL fine-tuning capability

------------------------------------------------------------------------

## ğŸ“š Datasets

### ğŸ”¹ HuggingFaceM4/WebSight

UI screenshots â†” HTML/CSS pairs

### ğŸ”¹ KingstarOMEGA/HTML-CSS-UI

HTML/CSS (renderable to UI)

### ğŸ”¹ Custom Dataset (Optional)

Generated via web scraping or rendering pipelines

**Note:** RL training only requires images (HTML optional).

------------------------------------------------------------------------

## ğŸ§ª Baselines

1.  Base model (no tuning)\
2.  Supervised Fine-Tuning (SFT)\
3.  RLVR-trained model

Evaluation:

-   Rendering similarity\
-   HTML validity\
-   Structural correctness

------------------------------------------------------------------------

## ğŸ“ Evaluation Metrics

### ğŸ¨ Visual Metrics

-   SSIM\
-   LPIPS\
-   Pixel accuracy

### ğŸ§± Structural Metrics

-   DOM similarity\
-   Tag validity rate

### ğŸ“ Layout Metrics

-   Block detection accuracy\
-   Reading order consistency

------------------------------------------------------------------------

## ğŸ–¥ Compute Resources

Training performed on:

-   **4 Ã— NVIDIA A100 (80GB)**

Supports:

âœ” RL fine-tuning\
âœ” Rendering-based reward loops\
âœ” Large VLM experimentation

------------------------------------------------------------------------

## âš™ï¸ Training Strategy

### Phase 1 --- (Optional) SFT

Train on UI â†” HTML/CSS pairs

### Phase 2 --- RLVR

Reward-based optimization using rendering similarity

------------------------------------------------------------------------

## ğŸš€ Setup

``` bash
git clone https://github.com/<repo>/rlvr-ui-codegen.git
cd rlvr-ui-codegen
pip install -r requirements.txt
```

------------------------------------------------------------------------

## â–¶ï¸ Usage

``` bash
python generate.py --model checkpoints/rlvr_model --image samples/ui_example.png
```

Output:

    output/
     â”œâ”€â”€ index.html
     â”œâ”€â”€ styles.css
     â””â”€â”€ render.png

------------------------------------------------------------------------

## ğŸ›£ Roadmap

-   [ ] Dataset curation\
-   [ ] SFT baseline\
-   [ ] Reward design\
-   [ ] RLVR training\
-   [ ] Evaluation framework\
-   [ ] Ablation studies

------------------------------------------------------------------------

## ğŸ‘¥ Team

**Amal Joe**\
**Job J**

------------------------------------------------------------------------

## ğŸ“– References

DeepSeek-R1\
DeepSeekMath / DeepSeekMath-V2\
Infinity Parser (LayoutRL)\
Efficient Medical VIE via RL\
Pix2Struct\
Nougat

------------------------------------------------------------------------

## ğŸ’¡ Key Research Questions

-   Does RLVR improve visual layout fidelity?\
-   Can rewards replace heavy supervision?\
-   Does RLVR reduce hallucinated elements?\
-   How stable is rendering-based RL training?

