{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c70f26a7-1793-4001-90c0-3aa8d6305380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOTrainer\n",
    "from trl import GRPOConfig\n",
    "from math_verify import LatexExtractionConfig, parse, verify\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from typing import Optional\n",
    "import re\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration\n",
    "from transformers import AutoProcessor\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac8b853e-5b32-4d6d-a183-2f0748f3b816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=716x200 at 0x7FE103959E10>, 'solution': \"<think>Let's examine the polynomial expressions given for each side of the triangle. The side labeled \\\\(4x^2 + x\\\\) does not have a constant term. The side labeled \\\\(2x + 3\\\\) has a constant term of 3. The side labeled \\\\(4x^3 + 2x^2 + 5\\\\) has a constant term of 5. To find the total constant term, we need to add the constant terms from these expressions. So, we add 3 and 5 together. 3 + 5 = 8</think>\\n\\n<answer>The correct answer is C</answer>\", 'prompt': [{'content': [{'text': 'A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>', 'type': 'text'}], 'role': 'system'}, {'content': [{'text': None, 'type': 'image'}, {'text': 'Based on the image, determine the constant term after combining all the polynomial expressions representing the side lengths of the triangle. Choose the correct answer from the options provided.\\n\\nChoices:\\nA. 3\\nB. 5\\nC. 8\\nD. 13', 'type': 'text'}], 'role': 'user'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset_id = 'lmms-lab/multimodal-open-r1-8k-verified'\n",
    "dataset = load_dataset(dataset_id, split='train[:5%]')\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True, padding_side=\"left\")\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
    "    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
    "    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
    "    \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
    ")\n",
    "\n",
    "def make_conversation(example):\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": SYSTEM_PROMPT},\n",
    "        ]},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": example[\"problem\"]},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    return {\n",
    "        \"prompt\": conversation,\n",
    "        \"image\": example[\"image\"],\n",
    "    }\n",
    "    \n",
    "train_dataset = train_dataset.map(make_conversation)\n",
    "train_dataset = train_dataset.remove_columns(['problem', 'original_question', 'original_answer'])\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1884fcb7-ff85-48ab-986b-941855781101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd6f18798d74b20be394d6e1fae73e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b622093501c941a699bc42ca9d5aabe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eedd966b8ac3459086389743867f0904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,843,200 || all params: 3,756,466,176 || trainable%: 0.0491\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70332c-c3f3-4220-a654-e94c6759c594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:173: FutureWarning: The `max_prompt_length` argument is deprecated and will be removed in version 0.28.0. You should instead filter your dataset before training to ensure that prompts do not exceed your desired length.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "Passing `generation_config` together with generation-related arguments=({'disable_compile'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/38 26:03 < 29:07, 0.01 it/s, Epoch 0.47/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.006441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.033680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.017411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.043256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.015104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.032190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.022660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.005277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.059910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.041505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>-0.007731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>-0.043667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.042257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.036387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>-0.008184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dev/shm/vcoder2/lib/python3.10/site-packages/peft/utils/other.py:1394: UserWarning: Unable to fetch remote file due to the following error [SSL] record layer failure (_ssl.c:1017) - silently ignoring the lookup for the file config.json in Qwen/Qwen2.5-VL-3B-Instruct.\n",
      "  warnings.warn(\n",
      "/dev/shm/vcoder2/lib/python3.10/site-packages/peft/utils/save_and_load.py:295: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-3B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<think>\\n.*?\\n</think>\\n<answer>\\n.*?\\n</answer>$\"\n",
    "    matches = [re.match(pattern, content[0]['content'], re.DOTALL | re.MULTILINE) for content in completions]\n",
    "    rewards = [1.0 if match else 0.0 for match in matches]\n",
    "    return rewards\n",
    "\n",
    "def accuracy_reward(completions: list[list[dict[str, str]]], solution: list[str], **kwargs) -> list[Optional[float]]:\n",
    "    \"\"\"Reward function that checks if the completion matches the ground truth.\n",
    "    - If both gold and prediction are parseable → use math verification.\n",
    "    - If not parseable → compare as normalized text.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for completion, sol in zip(completions, solution):\n",
    "        completion = completion[0]['content']\n",
    "        try:\n",
    "            gold_parsed = parse(sol, extraction_mode=\"first_match\")\n",
    "        except Exception as e:\n",
    "            gold_parsed = []\n",
    "\n",
    "        if len(gold_parsed) != 0:\n",
    "            # Try parsing predicted answer too\n",
    "            try:\n",
    "                answer_parsed = parse(\n",
    "                    completion,\n",
    "                    extraction_config=[\n",
    "                        LatexExtractionConfig(\n",
    "                            normalization_config=NormalizationConfig(\n",
    "                                nits=False,\n",
    "                                malformed_operators=False,\n",
    "                                basic_latex=True,\n",
    "                                boxed=\"all\",\n",
    "                                units=True,\n",
    "                            ),\n",
    "                            boxed_match_priority=0,\n",
    "                            try_extract_without_anchor=False,\n",
    "                        )\n",
    "                    ],\n",
    "                    extraction_mode=\"first_match\",\n",
    "                )\n",
    "                reward = float(verify(gold_parsed, answer_parsed))\n",
    "            except Exception as e:\n",
    "                print(f\"verify failed: {e}, answer: {completion}, gold: {sol}\")\n",
    "                reward = None\n",
    "        else:\n",
    "            # fallback to text match\n",
    "            reward = float(completion.strip().lower() == sol.strip().lower())\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "# Configure training arguments using GRPOConfig\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"Qwen2.5-VL-3B-Instruct-Thinking\",\n",
    "    learning_rate=1e-5,\n",
    "    remove_unused_columns=False, # to access the solution column in accuracy_reward\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "\n",
    "    # Parameters that control the data preprocessing\n",
    "    per_device_train_batch_size=64,\n",
    "    max_completion_length=256, # default: 256\n",
    "    num_generations=8, # default: 8\n",
    "    max_prompt_length=2048,\n",
    "\n",
    "    # Parameters related to reporting and saving\n",
    "    report_to=[\"tensorboard\"],\n",
    "    logging_steps=1,\n",
    "    push_to_hub=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=processor,\n",
    "    reward_funcs=[format_reward, accuracy_reward],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38688532-2eb7-4afb-84f7-cf6b032621c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
