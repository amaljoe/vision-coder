{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3-VL-2B GRPO Training: UI Screenshot â†’ HTML/CSS\n",
    "\n",
    "Train a Vision Language Model to generate HTML+Tailwind CSS from website screenshots using GRPO.\n",
    "\n",
    "- **Model**: Qwen3-VL-2B-Instruct (LoRA)\n",
    "- **Dataset**: WebSight v0.2 (1% subset)\n",
    "- **Rewards**: format (0.1), HTML validity (0.2), visual fidelity/SSIM (0.5), structural similarity (0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "\n",
    "from vcoder import (\n",
    "    format_reward,\n",
    "    html_validity_reward,\n",
    "    visual_fidelity_reward,\n",
    "    structural_similarity_reward,\n",
    ")\n",
    "from vcoder.data.websight import load_websight_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-VL-2B-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True, padding_side=\"left\")\n",
    "\n",
    "train_dataset = load_websight_dataset(processor, split=\"train[:1%]\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model + LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = GRPOConfig(\n",
    "    output_dir=\"Qwen3-VL-2B-HTMLCSS\",\n",
    "    learning_rate=5e-6,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "\n",
    "    # Batch / generation parameters\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    max_completion_length=2048,\n",
    "    num_generations=4,\n",
    "    max_prompt_length=4096,\n",
    "\n",
    "    # Reporting and saving\n",
    "    report_to=[\"tensorboard\"],\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    save_total_limit=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Trainer and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=processor,\n",
    "    reward_funcs=[\n",
    "        format_reward,\n",
    "        html_validity_reward,\n",
    "        visual_fidelity_reward,\n",
    "        structural_similarity_reward,\n",
    "    ],\n",
    "    reward_weights=[0.1, 0.2, 0.5, 0.2],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Final Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"Qwen3-VL-2B-HTMLCSS/final\")\n",
    "processor.save_pretrained(\"Qwen3-VL-2B-HTMLCSS/final\")\n",
    "print(\"Training complete. Adapter saved to Qwen3-VL-2B-HTMLCSS/final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TensorBoard\n",
    "\n",
    "Launch TensorBoard to monitor training:\n",
    "```bash\n",
    "tensorboard --logdir=./Qwen3-VL-2B-HTMLCSS/runs --host=0.0.0.0 --port=6006\n",
    "```\n",
    "\n",
    "Port forward from local machine:\n",
    "```bash\n",
    "ssh -L 6006:localhost:6006 cn17-dgx -p 4422\n",
    "```\n",
    "\n",
    "Key metrics:\n",
    "- `reward` - overall average (should trend up)\n",
    "- `rewards/visual_fidelity_reward/mean` - most important signal\n",
    "- `rewards/format_reward/mean` - should converge quickly to ~0.8-1.0\n",
    "- `completions/clipped_ratio` - should be <0.2\n",
    "- `completions/mean_length` - expect 500-1500 tokens\n",
    "- `entropy` - gradual decrease"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
